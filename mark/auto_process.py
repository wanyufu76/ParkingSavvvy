import os
import json
import shutil
import subprocess
from datetime import datetime
import time
import numpy as np
from supabase import create_client
from infer_location import infer_location_clip


SUPABASE_URL = "https://polqjhuklxclnvgpjckf.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InBvbHFqaHVrbHhjbG52Z3BqY2tmIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1MjI4MTA5NywiZXhwIjoyMDY3ODU3MDk3fQ.tA_l_KmEsm3YlnPfohlwaYiOG3fnTrbZRlJGUCpkWnk"
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

BASE_CONFIG_DIR = "base_config"
DOWNLOAD_DIR = "downloads"
os.makedirs(BASE_CONFIG_DIR, exist_ok=True)
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

# ----------------- å¹¾ä½•å‡½å¼ -----------------
def reorder_box_points(points):
    """æŠŠ 4 é»æ’åºæˆé †æ™‚é‡"""
    pts = np.array(points, dtype=float)
    center = np.mean(pts, axis=0)
    angles = np.arctan2(pts[:,1]-center[1], pts[:,0]-center[0])
    pts = pts[np.argsort(angles)]
    return pts

def align_points_to_centerline(points, box_points, padding_ratio=0.1, smooth_factor=0.3):
    """
    ç´…é»æ²¿è‘—åœè»Šä½ä¸­å¿ƒç·šæ’åˆ—ï¼Œå¸¶åŸå§‹è·é›¢æ¬Šé‡çš„å¹³æ»‘åˆ†å¸ƒ
    """
    box_points = reorder_box_points(box_points)

    # è¨ˆç®—é‚Šé•·
    edges = [(box_points[i], box_points[(i+1)%4]) for i in range(4)]
    lengths = [np.linalg.norm(e[1]-e[0]) for e in edges]

    # æ‰¾æœ€çŸ­é‚Šï¼ˆåœè»Šä½é ­å°¾ï¼‰
    short_idx = int(np.argmin(lengths))
    short_edge = edges[short_idx]
    opp_edge   = edges[(short_idx+2)%4]

    # ä¸­å¿ƒç·š
    center_start = (short_edge[0]+short_edge[1])/2
    center_end   = (opp_edge[0]+opp_edge[1])/2
    dir_vec = center_end - center_start
    dir_len = np.linalg.norm(dir_vec)
    dir_vec /= dir_len

    # 1ï¸âƒ£ åŸå§‹æŠ•å½±è·é›¢
    proj = [np.dot(pt-center_start, dir_vec) for pt in points]
    sorted_idx = np.argsort(proj)
    proj = np.array(proj)[sorted_idx]

    # 2ï¸âƒ£ åŸå§‹è·é›¢æ­£è¦åŒ–
    min_proj, max_proj = proj.min(), proj.max()
    proj_range = max_proj - min_proj if max_proj>min_proj else 1.0
    norm_proj = (proj - min_proj) / proj_range

    # 3ï¸âƒ£ ç·šæ€§ç­‰è· + åŸå§‹è·é›¢æ··åˆ
    n = len(points)
    linear = np.linspace(0, 1, n)
    mixed = (1-smooth_factor)*linear + smooth_factor*norm_proj

    # 4ï¸âƒ£ åŠ  padding
    start_pos = dir_len * padding_ratio
    end_pos   = dir_len * (1 - padding_ratio)
    usable_len = end_pos - start_pos
    aligned_proj = start_pos + mixed * usable_len

    # 5ï¸âƒ£ é‚„åŸç©ºé–“åº§æ¨™
    aligned_points = [center_start + dir_vec*p for p in aligned_proj]
    return np.array(aligned_points)

def pixel_to_latlng(x, y, cfg):
    """åº•åœ–åƒç´ è½‰ç¶“ç·¯åº¦"""
    if cfg["img_width"] == 0 or cfg["img_height"] == 0:
        return None, None
    lng = cfg["lng_min"] + (x / cfg["img_width"]) * (cfg["lng_max"] - cfg["lng_min"])
    lat = cfg["lat_max"] - (y / cfg["img_height"]) * (cfg["lat_max"] - cfg["lat_min"])
    return lat, lng

# ----------------- DB æ“ä½œ -----------------

def get_unprocessed_images_raw(limit=100):
    r = (
        supabase.table("image_uploads")
        .select("id, filename, created_at, inferred_area, processed")
        .eq("processed", False)
        .order("created_at", desc=True)
        .limit(limit)
        .execute()
    )
    rows = r.data or []
    # åªç•™åœ–ç‰‡æª”
    return [
        row for row in rows
        if isinstance(row.get("filename"), str)
        and row["filename"].lower().endswith((".jpg", ".jpeg", ".png"))
    ]

def download_image(filename):
    source_path = os.path.join(r"E:\ParkSavvy\uploads", filename)
    save_path = os.path.join(DOWNLOAD_DIR, filename)
    if not os.path.exists(source_path):
        print(f" æ‰¾ä¸åˆ°åœ–ç‰‡ï¼š{source_path}")
        return None
    shutil.copyfile(source_path, save_path)
    print(f" å·²è¤‡è£½åœ–ç‰‡ï¼š{filename}")
    return save_path

def mark_as_processed(image_id):
    supabase.table("image_uploads").update({"processed": True}).eq("id", image_id).execute()

def upload_motor_records(result_path, location, filename):
    """æ¸…ç©ºè©²å€ motor_recordsï¼Œæ’å…¥æœ€æ–°åµæ¸¬çµæœ"""
    with open(result_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    supabase.table("motor_records").delete().eq("location", location).execute()
    print(f"ğŸ§¹ å·²æ¸…é™¤ {location} å…ˆå‰ç´€éŒ„")

    records = []
    for item in data:
        records.append({
            "image_filename": filename,
            "motor_index": item["motor_index"],
            "location": location,
            "real_x": item["real_x"],  # åº•åœ–åƒç´ 
            "real_y": item["real_y"],
            "plate_text": item["plate_text"],
            "match_distance": item["match_distance"],
            "created_at": datetime.now().isoformat()
        })

    if records:
        supabase.table("motor_records").insert(records).execute()
        print(f"å·²ä¸Šå‚³ {len(records)} ç­†é…å°è³‡æ–™")
    else:
        print(" æ²’æœ‰é…å°è³‡æ–™å¯ä¸Šå‚³")

def upsert_current_count(area_id: str, count: int, src_id: str | None = None):
    payload = {
        "area_id": area_id,                # â† æ”¹é€™è£¡
        "scooter_count": int(count),
        "ts": datetime.now().isoformat(),
        "src_id": src_id or "",
    }
    supabase.table("current_status").upsert(
        payload,
        on_conflict="area_id"              # â† é€™è£¡ä¹Ÿæ”¹
    ).execute()

# ----------------- JSON ç”¢å‡º -----------------
def generate_json_for_location(location):
    """ç”¢ç”Ÿå‰ç«¯å¯ç”¨ JSONï¼Œç´…é»æ²¿ä¸­å¿ƒç·šæ•´é½ŠåŒ–ï¼Œå«ç¶“ç·¯åº¦"""
    # æœ€æ–°åœ–ç‰‡
    uploads = (
        supabase.table("image_uploads")
        .select("filename", "created_at")
        .eq("inferred_area", location)
        .order("created_at", desc=True)
        .limit(1)
        .execute()
    )
    if not uploads.data:
        print(f" {location} æ²’æœ‰ä»»ä½•åœ–ç‰‡ä¸Šå‚³ç´€éŒ„")
        return

    latest_filename = uploads.data[0]["filename"]

    # motor_records
    records = (
        supabase.table("motor_records")
        .select("*")
        .eq("image_filename", latest_filename)
        .execute()
        .data
    )
    if not records:
        print(f"âš ï¸ {location} æ²’æœ‰ motor_records")
        return

    # è®€å–åº•åœ–è¨­å®š
    box_data = (
        supabase.table("base_configs")
        .select("*")
        .eq("area_id", location)
        .limit(1)
        .execute()
        .data
    )
    if not box_data:
        print(f"âš ï¸ æ‰¾ä¸åˆ° {location} çš„åº•åœ–è³‡è¨Š")
        return

    cfg = box_data[0]
    coords = json.loads(cfg["coords"])
    # å°‡è—æ¡†è½‰ç‚ºåƒç´ åº§æ¨™
    box_points = np.array([
        [ (c["lng"]-cfg["lng_min"])/(cfg["lng_max"]-cfg["lng_min"])*cfg["img_width"],
          (cfg["lat_max"]-c["lat"])/(cfg["lat_max"]-cfg["lat_min"])*cfg["img_height"] ]
        for c in coords
    ], dtype=float)
    box_points = reorder_box_points(box_points)

    # éæ¿¾ä¸¦æ•´ç†ç´…é»
    points = []
    markers = []
    for item in records:
        x_px = item.get("real_x")
        y_px = item.get("real_y")
        if x_px is None or y_px is None:
            continue
        if not isinstance(x_px, (int,float)) or not isinstance(y_px, (int,float)):
            continue

        points.append([x_px, y_px])
        lat, lng = pixel_to_latlng(x_px, y_px, cfg)
        if lat is None or lng is None:
            continue

        markers.append({
            "motor_index": item["motor_index"],
            "plate_text": item["plate_text"],
            "pixel_x": int(x_px),
            "pixel_y": int(y_px),
            "lat": lat,
            "lng": lng,
            "location": location,
            "image_filename": item["image_filename"],
        })

    # ç´…é»æ²¿ä¸­å¿ƒç·šæ•´é½ŠåŒ–
    if points:
        aligned_pts = align_points_to_centerline(np.array(points, dtype=float), box_points)
        for i, p in enumerate(aligned_pts):
            x, y = p
            lat, lng = pixel_to_latlng(x, y, cfg)
            markers[i]["pixel_x"] = int(x)
            markers[i]["pixel_y"] = int(y)
            markers[i]["lat"] = lat
            markers[i]["lng"] = lng

    latest_count = len(markers)
    upsert_current_count(location, latest_count, src_id=latest_filename)
    # è¼¸å‡º JSON
    out = f"map_output_{location}.json"
    with open(out, "w", encoding="utf-8") as f:
        json.dump(markers, f, ensure_ascii=False, indent=2)
    print(f" {location}: å·²è¼¸å‡º {out} (å«ç¶“ç·¯åº¦ï¼Œæ²¿ä¸­å¿ƒç·šæ•´é½ŠåŒ–)")

# ----------------- ä¸»æµç¨‹ -----------------
if __name__ == "__main__":
    rows = get_unprocessed_images_raw()
    if not rows:
        print(" æ²’æœ‰æ–°çš„åœ–ç‰‡è¦è™•ç†")
    else:
        # å…ˆæŠŠæ²’æ¨è«–å€åŸŸçš„è£œä¸Š
        for row in rows:
            if not row.get("inferred_area"):
                image_id = row["id"]
                filename = row["filename"]
                local_path = download_image(filename)
                if local_path is None:
                    mark_as_processed(image_id)
                    continue
                try:
                    area = infer_location_clip(local_path)  # é€™è£¡å°±æœƒã€ŒçœŸçš„ã€è·‘åˆ°
                    print(f"åœ–ç‰‡å€åŸŸ(æ–°æ¨è«–): {area}")
                    supabase.table("image_uploads").update(
                        {"inferred_area": area}
                    ).eq("id", image_id).execute()
                    row["inferred_area"] = area
                except Exception as e:
                    print(f"å€åŸŸæ¨è«–å¤±æ•—ï¼š{e}")
                    mark_as_processed(image_id)
                    continue

        # ä¾å€åŸŸæŒ‘ã€Œæœ€æ–°ä¸€å¼µã€ä¾†åš YOLO + ä¸Šå‚³
        latest_by_area = {}
        for row in rows:
            area = row.get("inferred_area")
            if not area:
                continue
            ts = row.get("created_at", "")
            if area not in latest_by_area or ts > latest_by_area[area].get("created_at", ""):
                latest_by_area[area] = row

        targets = list(latest_by_area.values())
        if not targets:
            print(" æ²’æœ‰å®Œæˆå€åŸŸåˆ¤å®šçš„åœ–ç‰‡å¯è™•ç†")
        else:
            for img in targets:
                image_id = img["id"]
                filename = img["filename"]
                location = img["inferred_area"]

                print(f"\nè™•ç†åœ–ç‰‡: {filename} @ {location}")

                downloaded_path = download_image(filename)
                if downloaded_path is None:
                    mark_as_processed(image_id)
                    continue

                # OCR
                ocr_json_path = os.path.abspath(os.path.join(DOWNLOAD_DIR, f"{filename}_ocr.json"))
                subprocess.run([
                    "conda","run","-n","ocr_env","python", r"E:\ParkSavvy\mark\ocr.py",
                    downloaded_path, ocr_json_path
                ], check=False)

                # YOLO + Homography
                base_config_dir = f"base_config_{location}"
                subprocess.run([
                    "conda","run","-n","yolo_paddle","python", r"E:\ParkSavvy\mark\based_mark.py",
                    downloaded_path, base_config_dir, ocr_json_path
                ], check=False)

                # ä¸Šå‚³ motor_records
                result_json_path = downloaded_path + "_result.json"
                if os.path.exists(result_json_path):
                    upload_motor_records(result_json_path, location, filename)

                # é€™å¼µæ¨™ç‚º processedï¼Œä¸”æŠŠåŒå€èˆŠåœ–ä¹Ÿæ¨™ processed
                mark_as_processed(image_id)
                supabase.table("image_uploads").update(
                    {"processed": True}
                ).eq("inferred_area", location).neq("id", image_id).execute()

                # ç”¢ map_output_*.json
                generate_json_for_location(location)

        print("\nâœ… æ‰€æœ‰åœ°å€è™•ç†å®Œæˆï¼")